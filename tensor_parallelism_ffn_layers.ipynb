{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhGlDAqHuJwH3Y2/n0fm+u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puneeshkhanna/Tensor-Parallelism/blob/master/tensor_parallelism_ffn_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Pl8o6JNBXvL"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input of dimensions (batch size, no of words, embedding dimension of each word)\n",
        "input = torch.randn(size=(1, 5, 10), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "Td1StIsVCldQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = input.size(dim=2)\n",
        "embedding_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2ZqB8YLTnWI",
        "outputId": "9428a908-dfa0-4294-fbd2-ecbd8d38807d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to back linear layers output\n",
        "\n",
        "Tranformer architectures have back to back linear layers where the embedding dim\n",
        "first goes from h to 4h and then back from 4h to h\n",
        "\n",
        "```\n",
        "(mlp): MLP(\n",
        "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=False)          \n",
        "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=False)\n",
        "        )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "zdTszgCgW1zY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_h_to_4h = nn.Linear(in_features=embedding_dim, out_features=embedding_dim*4, bias=False)\n",
        "\n",
        "# Input of shape [1,5,10] * W.T of shape [10, 40]\n",
        "output = linear_h_to_4h(input)\n",
        "\n",
        "# input of dimension (1,5,10) gets transformed to dimension (1,5,40)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "SkVpKZr7BcCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f88a2c-fd6e-40e0-ca2c-5a97894b6670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 40])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linear_4h_to_h = nn.Linear(in_features=embedding_dim*4, out_features=embedding_dim, bias=False)\n",
        "\n",
        "# h_to_4h output of shape [1,5,40] * W.T of shape [40,10]\n",
        "final_output = linear_4h_to_h(output)\n",
        "\n",
        "# h_to_4h output of dimension (1,5,40) gets transformed back to dimension (1,5,10)\n",
        "final_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMDkkiqACDmd",
        "outputId": "429e536d-0cc4-4b41-85e1-24097eeafc50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Back to back linear layer outputs with Tensor parallelism\n",
        "\n",
        "Assuming 2 devices, below code depicts how weights will be divided between the 2 devices.\n",
        "\n",
        "Note that all the below code is executed on single device only with comments that which blocks will be executed on first device or second device."
      ],
      "metadata": {
        "id": "rINCD0tnWsFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_devices = 2\n",
        "weight_parallel = int((embedding_dim*4)/n_devices)\n",
        "weight_parallel"
      ],
      "metadata": {
        "id": "z871j0n6UijU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20d9318-7baa-4cb2-fa3b-e9deb230106c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First device - h_to_4h logic"
      ],
      "metadata": {
        "id": "mnzBoHpRUT5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First device weights of h_to_4h linear layer will be [out_features, in_feature] = [20, 10]\n",
        "linear_h_to_4h_parallel = nn.Linear(in_features=embedding_dim, out_features=weight_parallel, bias=False)\n",
        "\n",
        "# Set weights of this layer to the first 20 rows of the original weights\n",
        "linear_h_to_4h_parallel.weight.data = linear_h_to_4h.weight[:weight_parallel,:]\n",
        "\n",
        "# Input of shape [1,5,10] * W.T of shape [10, 20] ; note that in W.T - it is actually the columns which are divided; hence this is also known as column parallel linear\n",
        "output_parallel_1 = linear_h_to_4h_parallel(input)\n",
        "\n",
        "output_parallel_1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu5a_7vnCP4c",
        "outputId": "5769fac3-b2ae-49e2-f5c6-075eeda06fcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second device - h_to_4h logic"
      ],
      "metadata": {
        "id": "-GGjyS7AUae4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Second device weights of h_to_4h linear layer will be [out_features, in_feature] = [20, 10]\n",
        "linear_h_to_4h_parallel = nn.Linear(in_features=embedding_dim, out_features=weight_parallel, bias=False)\n",
        "\n",
        "# Set weights of this layer to the last 20 rows of the original weights\n",
        "linear_h_to_4h_parallel.weight.data = linear_h_to_4h.weight[weight_parallel:,:]\n",
        "\n",
        "# Input[1,5,10] * W.T[10, 20] ; note that in W.T - it is actually the columns which are divided; hence this is also known as column parallel linear\n",
        "output_parallel_2 = linear_h_to_4h_parallel(input)\n",
        "\n",
        "output_parallel_2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K-W6FiGEj8Q",
        "outputId": "6210e9ee-2e72-45e5-cd8f-9142d3890480"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we perform an all gather of output_parallel_1 and output_parallel_2, we will get the actual output of h_to_4h linear layer\n",
        "# but since we have one more linear layer of 4h_to_h, we can continue with the tensor parallel outputs."
      ],
      "metadata": {
        "id": "X4YjmSBoTEIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First device - 4h_to_h logic"
      ],
      "metadata": {
        "id": "ozgY5VOPUej6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First device weights of 4h_to_h linear layer will be [out_features, in_features] = [10, 20]\n",
        "linear_4h_to_h_parallel = nn.Linear(in_features=weight_parallel, out_features=embedding_dim, bias=False)\n",
        "\n",
        "# Set weights of this layer to the first 20 columns of the original weights\n",
        "linear_4h_to_h_parallel.weight.data = linear_4h_to_h.weight[:,:weight_parallel]\n",
        "\n",
        "# output_parallel_1 of shape [1,5,20] * W.T of shape [20,10] ; note that in W.T - it is actually the rows which are divided; hence this is also known as row parallel linear\n",
        "final_output_parallel_1 = linear_4h_to_h_parallel(output_parallel_1)"
      ],
      "metadata": {
        "id": "WYsWuO1sExXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second device - 4h_to_h logic"
      ],
      "metadata": {
        "id": "H2jEWsp3UiO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Second device weights of 4h_to_h linear layer will be [out_features, in_features] = [10, 20]\n",
        "linear_4h_to_h_parallel = nn.Linear(in_features=weight_parallel, out_features=embedding_dim, bias=False)\n",
        "\n",
        "# Set weights of this layer to the last 20 columns of the original weights\n",
        "linear_4h_to_h_parallel.weight.data = linear_4h_to_h.weight[:,weight_parallel:]\n",
        "\n",
        "# output_parallel_2 of shape [1,5,20] * W.T of shape [20,10] ; note that in W.T - it is actually the rows which are divided; hence this is also known as row parallel linear\n",
        "final_output_parallel_2 = linear_4h_to_h_parallel(output_parallel_2)"
      ],
      "metadata": {
        "id": "hb8qCCW7E0JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add tensor parallel outputs of 4h_to_h using torch.add; it will be an all reduce operation when using actual 2 devices"
      ],
      "metadata": {
        "id": "3ao8emKqUmgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In actual tensor parallelism between 2 devices, this will be all reduce operation\n",
        "final_output_tp = torch.add(final_output_parallel_1, final_output_parallel_2)\n",
        "final_output_tp.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcT8l7QGKpuO",
        "outputId": "1eea5c84-39b5-49a6-d395-50ba0cab38d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_output_tp.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-L9TVaaYIoYp",
        "outputId": "1a4195e7-e6c0-4972-8d96-41dcd6787f44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare final_output and final_output_tp using allclose"
      ],
      "metadata": {
        "id": "0Be7wx4XWowO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.allclose(final_output, final_output_tp, rtol=1e-05, atol=1e-05))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef3Ll13rIc6A",
        "outputId": "430927c4-017a-4205-aa66-c770e6866c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_output"
      ],
      "metadata": {
        "id": "51vlAHWkJHN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8720af21-5874-401b-f533-2a3f1da5aaf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.5751,  0.4617,  0.1493,  0.0056,  0.2218, -0.1540,  0.1138,\n",
              "          -0.3654,  0.7666, -0.3740],\n",
              "         [-0.3632,  0.2662,  0.1769, -0.1032,  0.0542, -0.2100,  0.2093,\n",
              "          -0.0711,  0.5230, -0.5673],\n",
              "         [ 0.0499,  0.1617, -0.1641, -0.1193,  0.0013,  0.0018, -0.0664,\n",
              "           0.0953,  0.0862, -0.1954],\n",
              "         [ 0.0680,  0.3866, -0.1191, -0.0693, -0.6244, -0.1078,  0.2118,\n",
              "           0.0076, -0.0895, -0.2661],\n",
              "         [ 0.3321, -0.2314,  0.3488, -0.1860,  0.3012, -0.4169, -0.4020,\n",
              "           0.0484, -0.1718,  0.3321]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_output_tp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SFJUEqKWIaS",
        "outputId": "317785ae-7169-4b9d-b79b-63ccbcf042e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.5751,  0.4617,  0.1493,  0.0056,  0.2218, -0.1540,  0.1138,\n",
              "          -0.3654,  0.7666, -0.3740],\n",
              "         [-0.3632,  0.2662,  0.1769, -0.1032,  0.0542, -0.2100,  0.2093,\n",
              "          -0.0711,  0.5230, -0.5673],\n",
              "         [ 0.0499,  0.1617, -0.1641, -0.1193,  0.0013,  0.0018, -0.0664,\n",
              "           0.0953,  0.0862, -0.1954],\n",
              "         [ 0.0680,  0.3866, -0.1191, -0.0693, -0.6244, -0.1078,  0.2118,\n",
              "           0.0076, -0.0895, -0.2661],\n",
              "         [ 0.3321, -0.2314,  0.3488, -0.1860,  0.3012, -0.4169, -0.4020,\n",
              "           0.0484, -0.1718,  0.3321]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fRIL27ZDWL0J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}