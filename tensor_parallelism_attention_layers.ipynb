{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOONHmXIE8gF/TGEhel/0Kp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puneeshkhanna/Tensor-Parallelism/blob/master/tensor_parallelism_attention_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7Pl8o6JNBXvL"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input of dimensions (batch size, no of words or q_len, embedding dimension or hidden size of each word)\n",
        "input = torch.randn(size=(1, 5, 32), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "Td1StIsVCldQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bsz, q_len, hidden_size = input.size()\n",
        "hidden_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2ZqB8YLTnWI",
        "outputId": "a0a6ea91-e616-43ea-f1ed-2cf37f906b7b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of attention heads of multi head attention of each transformer block\n",
        "num_heads = 4\n",
        "\n",
        "# hidden size is divisible by num heads; per head embedding dim\n",
        "head_dim = hidden_size // num_heads\n",
        "\n",
        "print(f\"num heads is {num_heads}, hidden size is {hidden_size}, head dim is {head_dim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCHcWBKFTeQ4",
        "outputId": "e2a9001f-609f-4dbf-b710-ecee55480b3f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num heads is 4, hidden size is 32, head dim is 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Layer output\n",
        "\n",
        "Tranformer architectures have Attention block with QKV layers followed by an o_proj (dense layer)\n",
        "\n",
        "```\n",
        "Attention(\n",
        "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "        )\n",
        "```"
      ],
      "metadata": {
        "id": "zdTszgCgW1zY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_proj = nn.Linear(hidden_size, num_heads * head_dim, bias=False)\n",
        "k_proj = nn.Linear(hidden_size, num_heads * head_dim, bias=False)\n",
        "v_proj = nn.Linear(hidden_size, num_heads * head_dim, bias=False)\n",
        "\n",
        "o_proj = nn.Linear(num_heads * head_dim, hidden_size, bias=False)\n",
        "\n",
        "print(\"q_proj weights shape:\", q_proj.weight.shape)\n",
        "print(\"o_proj weights shape:\", o_proj.weight.shape)"
      ],
      "metadata": {
        "id": "SkVpKZr7BcCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb996f1-53ac-4af8-fd28-e04bc10aecaf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q_proj weights shape: torch.Size([32, 32])\n",
            "o_proj weights shape: torch.Size([32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_states = q_proj(input)\n",
        "key_states = k_proj(input)\n",
        "value_states = v_proj(input)\n",
        "\n",
        "print(\"query_states after projections -> [batch size, q_len, hidden_size]: \", query_states.shape)\n",
        "\n",
        "query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "key_states = key_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "value_states = value_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "print(\"\\nquery_states after view and transpose -> [batch size, num heads, q_len, head_dim]:\", query_states.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMDkkiqACDmd",
        "outputId": "791281f4-55ce-43e0-b3b4-b6df1648047c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_states after projections -> [batch size, q_len, hidden_size]:  torch.Size([1, 5, 32])\n",
            "\n",
            "query_states after view and transpose -> [batch size, num heads, q_len, head_dim]: torch.Size([1, 4, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
        "attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "print(\"attn matrix after QK.T and softmax -> [batch size, num heads, q_len, q_len]:\", attn_weights.shape)\n",
        "\n",
        "attn_output = torch.matmul(attn_weights, value_states)\n",
        "print(\"\\nattn output -> matmul of attn matrix [batch size, num heads, q_len, q_len] and value states [batch size, num heads, q_len, head_dim] -> [batch size, num heads, q_len, head_dim]:\", attn_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1VgryJ3U08_",
        "outputId": "884da86b-f21f-44f1-81ba-42f28ae63997"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn matrix after QK.T and softmax -> [batch size, num heads, q_len, q_len]: torch.Size([1, 4, 5, 5])\n",
            "\n",
            "attn output -> matmul of attn matrix [batch size, num heads, q_len, q_len] and value states [batch size, num heads, q_len, head_dim] -> [batch size, num heads, q_len, head_dim]: torch.Size([1, 4, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "attn_output = attn_output.reshape(bsz, q_len, hidden_size)\n",
        "print(\"attn output after view and reshape -> [batch size, q_len, hidden_size]:\", attn_output.shape)\n",
        "\n",
        "attn_output_non_tp = o_proj(attn_output)\n",
        "print(\"\\nattn output after final o_proj -> [batch size, q_len, hidden_size]:\", attn_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B63TpefwVMGp",
        "outputId": "27e7956e-7404-4953-ba06-5864d1405ddb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn output after view and reshape -> [batch size, q_len, hidden_size]: torch.Size([1, 5, 32])\n",
            "\n",
            "attn output after final o_proj -> [batch size, q_len, hidden_size]: torch.Size([1, 5, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention layer output with Tensor parallelism\n",
        "\n",
        "Assuming 2 devices, below code depicts how attention heads will be divided between the 2 devices.\n",
        "\n",
        "Note that all the below code is executed on single device only with comments that which blocks will be executed on first device or second device."
      ],
      "metadata": {
        "id": "rINCD0tnWsFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_devices = 2\n",
        "num_heads = num_heads // n_devices\n",
        "hidden_size = hidden_size // n_devices\n",
        "print(f\"num heads changes to {num_heads}, hidden size changes to {hidden_size}\")"
      ],
      "metadata": {
        "id": "z871j0n6UijU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87b4b0b0-94ec-4ced-e51b-84b344506ac6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num heads changes to 2, hidden size changes to 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dividing q_proj, k_proj, v_proj, o_proj weights based upon num_heads\")\n",
        "\n",
        "query_slices = q_proj.weight.split(num_heads * head_dim, dim=0)\n",
        "\n",
        "key_slices = k_proj.weight.split(num_heads * head_dim, dim=0)\n",
        "\n",
        "value_slices = v_proj.weight.split(num_heads * head_dim, dim=0)\n",
        "\n",
        "o_proj_slices = o_proj.weight.split(num_heads * head_dim, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CoFzLL8XCyT",
        "outputId": "2a6571c0-5e44-41a5-dc9b-079cc24f7901"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dividing q_proj, k_proj, v_proj, o_proj weights based upon num_heads\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original qkv proj weights without TP:\",q_proj.weight.shape, \"\\tPer device qkv proj weights with TP:\", query_slices[0].shape, query_slices[1].shape)\n",
        "\n",
        "print(\"\\nOriginal o_proj weights without TP:\",o_proj.weight.shape, \"\\tPer device o_proj weights with TP:\", o_proj_slices[0].shape, o_proj_slices[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DrBkdTFs8sA",
        "outputId": "ecc94c30-f04a-4616-8bda-835cfc8cbff8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original qkv proj weights without TP: torch.Size([32, 32]) \tPer device qkv proj weights with TP: torch.Size([16, 32]) torch.Size([16, 32])\n",
            "\n",
            "Original o_proj weights without TP: torch.Size([32, 32]) \tPer device o_proj weights with TP: torch.Size([32, 16]) torch.Size([32, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First device - attention output logic"
      ],
      "metadata": {
        "id": "mnzBoHpRUT5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_states = F.linear(input, query_slices[0])\n",
        "\n",
        "key_states = F.linear(input, key_slices[0])\n",
        "\n",
        "value_states = F.linear(input, value_slices[0])\n",
        "\n",
        "print(\"query_states after projections -> [batch size, q_len, hidden_size]: \", query_states.shape)\n",
        "\n",
        "query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "key_states = key_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "value_states = value_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "print(\"\\nquery_states after view and transpose -> [batch size, num heads, q_len, head_dim]:\", query_states.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu5a_7vnCP4c",
        "outputId": "c97c8bdb-845f-4eaa-f112-cd6872f418f4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_states after projections -> [batch size, q_len, hidden_size]:  torch.Size([1, 5, 16])\n",
            "\n",
            "query_states after view and transpose -> [batch size, num heads, q_len, head_dim]: torch.Size([1, 2, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
        "\n",
        "attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "\n",
        "print(\"attn matrix after QK.T and softmax -> [batch size, num heads, q_len, q_len]:\", attn_weights.shape)\n",
        "\n",
        "attn_output = torch.matmul(attn_weights, value_states)\n",
        "\n",
        "print(\"attn output -> matmul of attn matrix [batch size, num heads, q_len, q_len] and value states [batch size, num heads, q_len, head_dim] -> [batch size, num heads, q_len, head_dim]:\", attn_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJQZB4XJX4NM",
        "outputId": "d574e105-39c5-41dd-c372-722904ad49ce"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn matrix after QK.T and softmax -> [batch size, num heads, q_len, q_len]: torch.Size([1, 2, 5, 5])\n",
            "attn output -> matmul of attn matrix [batch size, num heads, q_len, q_len] and value states [batch size, num heads, q_len, head_dim] -> [batch size, num heads, q_len, head_dim]: torch.Size([1, 2, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "\n",
        "attn_output = attn_output.reshape(bsz, q_len, hidden_size)\n",
        "\n",
        "print(\"attn output after view and reshape -> [batch size, q_len, hidden_size]:\", attn_output.shape)\n",
        "\n",
        "attn_output_1 = F.linear(attn_output, o_proj_slices[0])\n",
        "\n",
        "print(\"\\nattn output on 1st Tensor Parallel device after final o_proj -> [batch size, q_len, hidden_size]:\", attn_output_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kiviet3gX5Bl",
        "outputId": "77bf359b-0912-4ff5-e0f9-99bd586558b1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn output after view and reshape -> [batch size, q_len, hidden_size]: torch.Size([1, 5, 16])\n",
            "\n",
            "attn output on 1st Tensor Parallel device after final o_proj -> [batch size, q_len, hidden_size]: torch.Size([1, 5, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second device - attention output logic"
      ],
      "metadata": {
        "id": "-GGjyS7AUae4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_states = F.linear(input, query_slices[1])\n",
        "\n",
        "key_states = F.linear(input, key_slices[1])\n",
        "\n",
        "value_states = F.linear(input, value_slices[1])\n",
        "\n",
        "print(\"query_states after projections -> [batch size, q_len, hidden_size]: \", query_states.shape)\n",
        "\n",
        "query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "key_states = key_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "value_states = value_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "print(\"\\nquery_states after view and transpose -> [batch size, num heads, q_len, head_dim]:\", query_states.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K-W6FiGEj8Q",
        "outputId": "07c9bb9a-4138-4fb4-844d-b153d87805da"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_states after projections -> [batch size, q_len, hidden_size]:  torch.Size([1, 5, 16])\n",
            "\n",
            "query_states after view and transpose -> [batch size, num heads, q_len, head_dim]: torch.Size([1, 2, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
        "\n",
        "attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "\n",
        "print(\"attn matrix after QK.T and softmax -> [batch size, num heads, q_len, q_len]:\", attn_weights.shape)\n",
        "\n",
        "attn_output = torch.matmul(attn_weights, value_states)\n",
        "\n",
        "print(\"\\nattn output -> matmul of attn matrix [batch size, num heads, q_len, q_len] and value states [batch size, num heads, q_len, head_dim] -> [batch size, num heads, q_len, head_dim]:\", attn_output.shape)"
      ],
      "metadata": {
        "id": "X4YjmSBoTEIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57e1074-4c94-4736-c7d8-0496aa12d2cf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn matrix after QK.T and softmax -> [batch size, num heads, q_len, q_len]: torch.Size([1, 2, 5, 5])\n",
            "\n",
            "attn output -> matmul of attn matrix [batch size, num heads, q_len, q_len] and value states [batch size, num heads, q_len, head_dim] -> [batch size, num heads, q_len, head_dim]: torch.Size([1, 2, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "\n",
        "attn_output = attn_output.reshape(bsz, q_len, hidden_size)\n",
        "\n",
        "print(\"attn output after view and reshape -> [batch size, q_len, hidden_size]:\", attn_output.shape)\n",
        "\n",
        "attn_output_2 = F.linear(attn_output, o_proj_slices[1])\n",
        "\n",
        "print(\"\\nattn output on 2nd Tensor Parallel device after final o_proj -> [batch size, q_len, hidden_size]:\", attn_output_2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUzzjciyYuLs",
        "outputId": "e9274469-88c4-4899-9bd8-b08a566ad069"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn output after view and reshape -> [batch size, q_len, hidden_size]: torch.Size([1, 5, 16])\n",
            "\n",
            "attn output on 2nd Tensor Parallel device after final o_proj -> [batch size, q_len, hidden_size]: torch.Size([1, 5, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add tensor parallel outputs of attn_output using torch.add; it will be an all reduce operation when using actual 2 devices"
      ],
      "metadata": {
        "id": "3ao8emKqUmgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In actual tensor parallelism between 2 devices, this will be all reduce operation\n",
        "attn_output_tp = torch.add(attn_output_1, attn_output_2)\n",
        "attn_output_tp.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcT8l7QGKpuO",
        "outputId": "b8e86b88-9ad2-42b1-9400-4fc223c8012f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare final_output_non_tp and final_output_tp using allclose"
      ],
      "metadata": {
        "id": "0Be7wx4XWowO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.allclose(attn_output_non_tp, attn_output_tp, rtol=1e-05, atol=1e-05))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef3Ll13rIc6A",
        "outputId": "b9c0ecfd-4476-4380-9316-f48cdae4d211"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output_non_tp"
      ],
      "metadata": {
        "id": "51vlAHWkJHN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbae3929-2f8d-4635-cd6c-b6741f9e338d"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 5.3712e-02, -1.1881e-01,  1.5588e-01,  5.6367e-02,  3.2628e-02,\n",
              "           7.5863e-02,  2.8828e-02,  7.3524e-02, -5.9104e-03,  5.6621e-02,\n",
              "           9.0149e-02,  6.6041e-02,  6.0233e-02,  5.9664e-02,  7.0160e-02,\n",
              "           1.8019e-01,  1.8662e-02,  3.4610e-02,  2.2844e-01, -1.4812e-01,\n",
              "          -1.2527e-02, -1.0309e-01,  8.6435e-02, -2.1808e-01,  1.4488e-01,\n",
              "          -1.4784e-01,  1.8542e-02, -7.8376e-02,  2.8780e-03, -2.3423e-01,\n",
              "          -3.8625e-02,  1.1624e-01],\n",
              "         [ 1.1160e-02, -1.0493e-01,  4.8735e-02,  1.5505e-01,  2.0439e-01,\n",
              "           1.7308e-03, -1.0093e-01,  1.0066e-01, -1.3819e-01,  3.0193e-01,\n",
              "          -1.6863e-02, -6.8167e-02,  1.3596e-01, -2.5812e-02,  1.3117e-01,\n",
              "           2.2190e-01,  8.5406e-02,  1.7488e-01,  2.2445e-01, -2.7848e-02,\n",
              "           1.9271e-02, -1.6363e-01,  2.3058e-02, -2.0312e-01,  1.2977e-01,\n",
              "          -6.3400e-02,  1.4039e-01,  1.8939e-01,  7.7873e-02,  4.7048e-02,\n",
              "          -1.1575e-01,  1.8809e-02],\n",
              "         [ 2.2801e-01,  7.1656e-02,  1.3488e-01,  1.1542e-01,  1.8698e-01,\n",
              "           4.7524e-02, -4.8382e-02,  1.5939e-01, -7.2792e-02,  2.1481e-01,\n",
              "           2.4715e-02, -1.4279e-01,  1.8525e-01,  2.6966e-02,  6.0645e-05,\n",
              "           2.8428e-01,  2.0060e-01, -2.8591e-02,  1.2529e-01, -5.8050e-03,\n",
              "           3.6741e-02, -1.0281e-01,  6.6173e-02, -1.5243e-01,  1.1673e-02,\n",
              "          -8.9311e-02,  1.9138e-01,  3.1176e-02, -5.5849e-02, -1.2097e-01,\n",
              "          -7.9183e-02,  2.7065e-02],\n",
              "         [ 9.5503e-02, -2.2877e-02,  6.0940e-02,  4.5235e-02, -4.8756e-02,\n",
              "           1.3383e-01,  4.3955e-02,  5.9662e-02,  2.1065e-02, -6.5123e-02,\n",
              "           8.9013e-03,  6.5458e-02, -1.4869e-02,  1.5004e-02, -8.7520e-02,\n",
              "           8.6569e-02, -3.7281e-02,  1.2934e-02,  1.2373e-01, -3.0402e-01,\n",
              "           3.4913e-02, -1.3117e-01,  1.8030e-02, -2.2615e-01,  1.6363e-01,\n",
              "          -1.3695e-01,  6.0634e-02, -2.7644e-01,  1.1046e-01, -1.6511e-01,\n",
              "           1.5837e-02,  1.4937e-01],\n",
              "         [ 9.7523e-02, -1.7086e-01,  7.4437e-02,  1.4227e-01,  1.4216e-02,\n",
              "           4.7822e-02, -5.3936e-03,  1.4199e-01,  7.6704e-02,  8.2866e-02,\n",
              "          -3.8727e-02,  1.5928e-01, -1.0915e-02, -4.0929e-02, -7.8880e-03,\n",
              "           1.1886e-01,  6.3708e-02,  2.1065e-03,  1.9123e-01, -1.8457e-01,\n",
              "           9.7050e-02, -6.1693e-02,  7.6741e-02, -2.2977e-01,  8.9148e-02,\n",
              "          -1.6761e-01,  1.0992e-01, -7.3945e-02,  1.0749e-01, -1.5866e-01,\n",
              "          -2.1946e-02,  1.5397e-01]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output_tp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SFJUEqKWIaS",
        "outputId": "56342514-7b3c-43a4-bf39-e448758315ca"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 5.3712e-02, -1.1881e-01,  1.5588e-01,  5.6367e-02,  3.2628e-02,\n",
              "           7.5863e-02,  2.8828e-02,  7.3524e-02, -5.9104e-03,  5.6621e-02,\n",
              "           9.0149e-02,  6.6041e-02,  6.0233e-02,  5.9664e-02,  7.0160e-02,\n",
              "           1.8019e-01,  1.8662e-02,  3.4610e-02,  2.2844e-01, -1.4812e-01,\n",
              "          -1.2527e-02, -1.0309e-01,  8.6435e-02, -2.1808e-01,  1.4488e-01,\n",
              "          -1.4784e-01,  1.8542e-02, -7.8376e-02,  2.8780e-03, -2.3423e-01,\n",
              "          -3.8625e-02,  1.1624e-01],\n",
              "         [ 1.1160e-02, -1.0493e-01,  4.8735e-02,  1.5505e-01,  2.0439e-01,\n",
              "           1.7308e-03, -1.0093e-01,  1.0066e-01, -1.3819e-01,  3.0193e-01,\n",
              "          -1.6863e-02, -6.8167e-02,  1.3596e-01, -2.5812e-02,  1.3117e-01,\n",
              "           2.2190e-01,  8.5406e-02,  1.7488e-01,  2.2445e-01, -2.7848e-02,\n",
              "           1.9271e-02, -1.6363e-01,  2.3058e-02, -2.0312e-01,  1.2977e-01,\n",
              "          -6.3400e-02,  1.4039e-01,  1.8939e-01,  7.7873e-02,  4.7048e-02,\n",
              "          -1.1575e-01,  1.8809e-02],\n",
              "         [ 2.2801e-01,  7.1656e-02,  1.3487e-01,  1.1542e-01,  1.8698e-01,\n",
              "           4.7524e-02, -4.8382e-02,  1.5939e-01, -7.2792e-02,  2.1481e-01,\n",
              "           2.4715e-02, -1.4279e-01,  1.8525e-01,  2.6966e-02,  6.0648e-05,\n",
              "           2.8428e-01,  2.0060e-01, -2.8591e-02,  1.2529e-01, -5.8050e-03,\n",
              "           3.6741e-02, -1.0281e-01,  6.6173e-02, -1.5243e-01,  1.1673e-02,\n",
              "          -8.9311e-02,  1.9138e-01,  3.1176e-02, -5.5849e-02, -1.2097e-01,\n",
              "          -7.9183e-02,  2.7065e-02],\n",
              "         [ 9.5503e-02, -2.2877e-02,  6.0940e-02,  4.5235e-02, -4.8756e-02,\n",
              "           1.3383e-01,  4.3955e-02,  5.9662e-02,  2.1065e-02, -6.5123e-02,\n",
              "           8.9013e-03,  6.5458e-02, -1.4869e-02,  1.5004e-02, -8.7520e-02,\n",
              "           8.6569e-02, -3.7281e-02,  1.2934e-02,  1.2373e-01, -3.0402e-01,\n",
              "           3.4913e-02, -1.3117e-01,  1.8030e-02, -2.2615e-01,  1.6363e-01,\n",
              "          -1.3695e-01,  6.0634e-02, -2.7644e-01,  1.1046e-01, -1.6511e-01,\n",
              "           1.5837e-02,  1.4937e-01],\n",
              "         [ 9.7523e-02, -1.7086e-01,  7.4437e-02,  1.4227e-01,  1.4216e-02,\n",
              "           4.7822e-02, -5.3936e-03,  1.4199e-01,  7.6704e-02,  8.2866e-02,\n",
              "          -3.8727e-02,  1.5928e-01, -1.0915e-02, -4.0929e-02, -7.8880e-03,\n",
              "           1.1886e-01,  6.3708e-02,  2.1065e-03,  1.9123e-01, -1.8457e-01,\n",
              "           9.7050e-02, -6.1693e-02,  7.6741e-02, -2.2977e-01,  8.9148e-02,\n",
              "          -1.6761e-01,  1.0992e-01, -7.3945e-02,  1.0749e-01, -1.5866e-01,\n",
              "          -2.1946e-02,  1.5397e-01]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}