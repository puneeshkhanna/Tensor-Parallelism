{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlz9+A7QRsc53RjtYBW5t0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puneeshkhanna/Tensor-Parallelism/blob/master/tensor_parallelism_attention_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Link - https://medium.com/@puneesh.khanna83/understanding-tensor-parallelism-to-fit-larger-models-on-multiple-devices-part-2-ee8a2ab7f017"
      ],
      "metadata": {
        "id": "V-6kqUI-9sPx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7Pl8o6JNBXvL"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input of dimensions (batch size, no of words or q_len, embedding dimension or hidden size of each word)\n",
        "input = torch.randn(size=(1, 5, 32), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "Td1StIsVCldQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bsz, q_len, hidden_size = input.size()\n",
        "hidden_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2ZqB8YLTnWI",
        "outputId": "0b161fc4-6bd1-4756-b920-b8cc68def855"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of attention heads of multi head attention of each transformer block\n",
        "num_heads = 4\n",
        "\n",
        "# hidden size must be divisible by num heads; calculate per head embedding dim\n",
        "head_dim = hidden_size // num_heads\n",
        "\n",
        "print(f\"num heads is {num_heads}, hidden size is {hidden_size}, head dim is {head_dim}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCHcWBKFTeQ4",
        "outputId": "d18ca3fc-d07e-4e7c-a907-f76c619a88e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num heads is 4, hidden size is 32, head dim is 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Layer output\n",
        "\n",
        "Tranformer architectures have Attention block with QKV layers followed by an o_proj (dense layer)\n",
        "\n",
        "```\n",
        "Attention(\n",
        "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
        "        )\n",
        "```\n",
        "\n",
        "In this notebook, below dimensions are used along with num_heads=4.\n",
        "```\n",
        "Attention(\n",
        "          (q_proj): Linear(in_features=32, out_features=32, bias=False)\n",
        "          (k_proj): Linear(in_features=32, out_features=32, bias=False)\n",
        "          (v_proj): Linear(in_features=32, out_features=32, bias=False)\n",
        "          (o_proj): Linear(in_features=32, out_features=32, bias=False)\n",
        "        )\n",
        "```"
      ],
      "metadata": {
        "id": "zdTszgCgW1zY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_proj = nn.Linear(hidden_size, num_heads * head_dim, bias=False)\n",
        "k_proj = nn.Linear(hidden_size, num_heads * head_dim, bias=False)\n",
        "v_proj = nn.Linear(hidden_size, num_heads * head_dim, bias=False)\n",
        "\n",
        "o_proj = nn.Linear(num_heads * head_dim, hidden_size, bias=False)\n",
        "\n",
        "print(\"q_proj, k_proj, v_proj weights.T shape:\", q_proj.weight.T.shape)\n",
        "print(\"o_proj weights.T shape:\", o_proj.weight.T.shape)"
      ],
      "metadata": {
        "id": "SkVpKZr7BcCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10d0e77c-e7bc-4f9d-bc5c-cc2f74814649"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "q_proj, k_proj, v_proj weights.T shape: torch.Size([32, 32])\n",
            "o_proj weights.T shape: torch.Size([32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_states = q_proj(input)\n",
        "key_states = k_proj(input)\n",
        "value_states = v_proj(input)\n",
        "\n",
        "print(\"query_states, key_states, value_states after projections -> [batch size, q_len, num heads*head_dim]: \", query_states.shape)\n",
        "\n",
        "query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "key_states = key_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "value_states = value_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "print(\"\\nquery_states, key_states, value_states after view and transpose -> [batch size, num heads, q_len, head_dim]:\", query_states.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMDkkiqACDmd",
        "outputId": "401c5729-cc87-4bec-920d-8d3d82ddfcdf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_states, key_states, value_states after projections -> [batch size, q_len, num heads*head_dim]:  torch.Size([1, 5, 32])\n",
            "\n",
            "query_states, key_states, value_states after view and transpose -> [batch size, num heads, q_len, head_dim]: torch.Size([1, 4, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
        "attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "print(\"attn scores after QK.T and softmax -> [batch size, num heads, q_len, q_len]:\", attn_weights.shape)\n",
        "\n",
        "attn_output = torch.matmul(attn_weights, value_states)\n",
        "print(\"\\nattn output -> matmul of attn scores [batch size, num heads, q_len, q_len] and value states [batch size, num heads, q_len, head_dim] -> [batch size, num heads, q_len, head_dim]:\", attn_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1VgryJ3U08_",
        "outputId": "9af0e7f1-399c-488e-e72a-fdb0c819db22"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn scores after QK.T and softmax -> [batch size, num heads, q_len, q_len]: torch.Size([1, 4, 5, 5])\n",
            "\n",
            "attn output -> matmul of attn scores [batch size, num heads, q_len, q_len] and value states [batch size, num heads, q_len, head_dim] -> [batch size, num heads, q_len, head_dim]: torch.Size([1, 4, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "attn_output = attn_output.reshape(bsz, q_len, num_heads*head_dim)\n",
        "print(\"attn output after view and reshape -> [batch size, q_len, num heads*head_dim]:\", attn_output.shape)\n",
        "\n",
        "attn_output_non_tp = o_proj(attn_output)\n",
        "print(\"\\nattn output after final o_proj -> [batch size, q_len, hidden_size]:\", attn_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B63TpefwVMGp",
        "outputId": "f8d55829-25c7-4a30-8ba5-4d8b51505272"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn output after view and reshape -> [batch size, q_len, num heads*head_dim]: torch.Size([1, 5, 32])\n",
            "\n",
            "attn output after final o_proj -> [batch size, q_len, hidden_size]: torch.Size([1, 5, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention layer output with Tensor parallelism\n",
        "\n",
        "Assuming 2 devices, below code depicts how attention heads will be divided between the 2 devices.\n",
        "\n",
        "Note that all the below code is executed on single device only with comments that which blocks will be executed on first device or second device."
      ],
      "metadata": {
        "id": "rINCD0tnWsFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_devices = 2\n",
        "num_heads = num_heads // n_devices\n",
        "print(f\"num heads changes to {num_heads} for each of the 2 device\")"
      ],
      "metadata": {
        "id": "z871j0n6UijU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e36d9d06-5751-46d1-d546-1515114a7923"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num heads changes to 2 for each of the 2 device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dividing q_proj, k_proj, v_proj, o_proj weights based upon num_heads\")\n",
        "\n",
        "query_slices = q_proj.weight.split(num_heads * head_dim, dim=0)\n",
        "key_slices = k_proj.weight.split(num_heads * head_dim, dim=0)\n",
        "value_slices = v_proj.weight.split(num_heads * head_dim, dim=0)\n",
        "\n",
        "o_proj_slices = o_proj.weight.split(num_heads * head_dim, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CoFzLL8XCyT",
        "outputId": "cf26febb-3f0c-4808-e4cc-f44bbd592625"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dividing q_proj, k_proj, v_proj, o_proj weights based upon num_heads\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original qkv proj weights transpose without TP:\",q_proj.weight.T.shape, \"\\tPer device qkv proj weights transpose with TP (column divided):\", query_slices[0].T.shape)\n",
        "\n",
        "print(\"\\nOriginal o_proj weights transpose without TP:\",o_proj.weight.T.shape, \"\\tPer device o_proj weights transpose with TP (row divided):\", o_proj_slices[0].T.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DrBkdTFs8sA",
        "outputId": "e6557e53-500f-42d0-ac92-8699fee79347"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original qkv proj weights transpose without TP: torch.Size([32, 32]) \tPer device qkv proj weights transpose with TP (column divided): torch.Size([32, 16])\n",
            "\n",
            "Original o_proj weights transpose without TP: torch.Size([32, 32]) \tPer device o_proj weights transpose with TP (row divided): torch.Size([16, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First device - attention output logic"
      ],
      "metadata": {
        "id": "mnzBoHpRUT5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_states = F.linear(input, query_slices[0])\n",
        "key_states = F.linear(input, key_slices[0])\n",
        "value_states = F.linear(input, value_slices[0])\n",
        "\n",
        "print(\"query_states, key_states, value_states after projections -> [batch size, q_len, num heads*head_dim]: \", query_states.shape)\n",
        "\n",
        "query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "key_states = key_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "value_states = value_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "print(\"\\nquery_states, key_states, value_states after view and transpose -> [batch size, num heads, q_len, head_dim]:\", query_states.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cu5a_7vnCP4c",
        "outputId": "11681bb9-a19d-43a4-d0b5-b46f7fae0dd4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_states, key_states, value_states after projections -> [batch size, q_len, num heads*head_dim]:  torch.Size([1, 5, 16])\n",
            "\n",
            "query_states, key_states, value_states after view and transpose -> [batch size, num heads, q_len, head_dim]: torch.Size([1, 2, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
        "attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "print(\"attn scores after QK.T and softmax -> [batch size, num heads, q_len, q_len]:\", attn_weights.shape)\n",
        "\n",
        "attn_output = torch.matmul(attn_weights, value_states)\n",
        "print(\"\\nattn output -> matmul of attn scores [batch size, num heads, q_len, q_len] and value states [batch size, num heads, q_len, head_dim] -> [batch size, num heads, q_len, head_dim]:\", attn_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJQZB4XJX4NM",
        "outputId": "9af76822-d99d-4804-e0e2-268c467841b4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn scores after QK.T and softmax -> [batch size, num heads, q_len, q_len]: torch.Size([1, 2, 5, 5])\n",
            "\n",
            "attn output -> matmul of attn scores [batch size, num heads, q_len, q_len] and value states [batch size, num heads, q_len, head_dim] -> [batch size, num heads, q_len, head_dim]: torch.Size([1, 2, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "attn_output = attn_output.reshape(bsz, q_len, num_heads*head_dim)\n",
        "print(\"attn output after view and reshape -> [batch size, q_len, num heads*head_dim]:\", attn_output.shape)\n",
        "\n",
        "attn_output_1 = F.linear(attn_output, o_proj_slices[0])\n",
        "print(\"\\nattn output on 1st Tensor Parallel device after final o_proj -> [batch size, q_len, hidden_size]:\", attn_output_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kiviet3gX5Bl",
        "outputId": "b79ebd31-8474-4a46-866a-b5527f5e03f7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn output after view and reshape -> [batch size, q_len, num heads*head_dim]: torch.Size([1, 5, 16])\n",
            "\n",
            "attn output on 1st Tensor Parallel device after final o_proj -> [batch size, q_len, hidden_size]: torch.Size([1, 5, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second device - attention output logic"
      ],
      "metadata": {
        "id": "-GGjyS7AUae4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_states = F.linear(input, query_slices[1])\n",
        "key_states = F.linear(input, key_slices[1])\n",
        "value_states = F.linear(input, value_slices[1])\n",
        "print(\"query_states, key_states, value_states after projections -> [batch size, q_len, num heads*head_dim]: \", query_states.shape)\n",
        "\n",
        "query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "key_states = key_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "value_states = value_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
        "print(\"\\nquery_states, key_states, value_states after view and transpose -> [batch size, num heads, q_len, head_dim]:\", query_states.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1K-W6FiGEj8Q",
        "outputId": "8063afae-2f50-47c7-e3ce-3756733c7f01"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "query_states, key_states, value_states after projections -> [batch size, q_len, num heads*head_dim]:  torch.Size([1, 5, 16])\n",
            "\n",
            "query_states, key_states, value_states after view and transpose -> [batch size, num heads, q_len, head_dim]: torch.Size([1, 2, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
        "attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
        "print(\"attn scores after QK.T and softmax -> [batch size, num heads, q_len, q_len]:\", attn_weights.shape)\n",
        "\n",
        "attn_output = torch.matmul(attn_weights, value_states)\n",
        "print(\"\\nattn output -> matmul of attn scores [batch size, num heads, q_len, q_len] and value states [batch size, num heads, q_len, head_dim] -> [batch size, num heads, q_len, head_dim]:\", attn_output.shape)"
      ],
      "metadata": {
        "id": "X4YjmSBoTEIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0208683-7443-474a-d66b-e67221c841ca"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn scores after QK.T and softmax -> [batch size, num heads, q_len, q_len]: torch.Size([1, 2, 5, 5])\n",
            "\n",
            "attn output -> matmul of attn scores [batch size, num heads, q_len, q_len] and value states [batch size, num heads, q_len, head_dim] -> [batch size, num heads, q_len, head_dim]: torch.Size([1, 2, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "attn_output = attn_output.reshape(bsz, q_len, num_heads*head_dim)\n",
        "print(\"attn output after view and reshape -> [batch size, q_len, num heads*head_dim]:\", attn_output.shape)\n",
        "\n",
        "attn_output_2 = F.linear(attn_output, o_proj_slices[1])\n",
        "print(\"\\nattn output on 2nd Tensor Parallel device after final o_proj -> [batch size, q_len, hidden_size]:\", attn_output_2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUzzjciyYuLs",
        "outputId": "5bf22ffb-63c9-4610-97f7-7f7954643e16"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attn output after view and reshape -> [batch size, q_len, num heads*head_dim]: torch.Size([1, 5, 16])\n",
            "\n",
            "attn output on 2nd Tensor Parallel device after final o_proj -> [batch size, q_len, hidden_size]: torch.Size([1, 5, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add tensor parallel outputs of attn_output using torch.add; it will be an all reduce operation when using actual 2 devices"
      ],
      "metadata": {
        "id": "3ao8emKqUmgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In actual tensor parallelism between 2 devices, this will be all reduce operation\n",
        "attn_output_tp = torch.add(attn_output_1, attn_output_2)\n",
        "attn_output_tp.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcT8l7QGKpuO",
        "outputId": "c3518348-cba1-4e04-d476-59a05fe20e21"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compare final_output_non_tp and final_output_tp using allclose"
      ],
      "metadata": {
        "id": "0Be7wx4XWowO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.allclose(attn_output_non_tp, attn_output_tp, rtol=1e-05, atol=1e-05))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef3Ll13rIc6A",
        "outputId": "f320edd2-4857-4536-a4bf-845eb7f0f24a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output_non_tp"
      ],
      "metadata": {
        "id": "51vlAHWkJHN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a39646d-face-4e58-c8da-77da33acfc67"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0735,  0.1547, -0.0292,  0.0470, -0.0282,  0.0597,  0.1018,\n",
              "           0.1307, -0.1112, -0.1699,  0.2083, -0.1083,  0.0874, -0.0449,\n",
              "          -0.1183, -0.2595, -0.0655, -0.2183,  0.1065, -0.1037,  0.2390,\n",
              "           0.0212,  0.0796, -0.0258,  0.1713,  0.0254,  0.1817, -0.1307,\n",
              "          -0.1476, -0.0997, -0.0394, -0.0292],\n",
              "         [-0.1030,  0.1681, -0.0376,  0.0200, -0.1233,  0.0678,  0.1338,\n",
              "          -0.0023, -0.1671, -0.1551,  0.1568, -0.0524, -0.0250, -0.0711,\n",
              "          -0.2904, -0.2652, -0.0289, -0.2211,  0.0744, -0.0724,  0.2301,\n",
              "           0.0566,  0.0180, -0.0022,  0.0772,  0.1041,  0.1453, -0.1092,\n",
              "          -0.1428,  0.0365,  0.0212, -0.0211],\n",
              "         [-0.1740,  0.0882, -0.0542, -0.0338, -0.0964,  0.1371,  0.0987,\n",
              "           0.0847, -0.2318, -0.1612,  0.1891, -0.0056, -0.0497, -0.1431,\n",
              "          -0.2468, -0.3641,  0.0449, -0.1987,  0.0127, -0.1358,  0.3252,\n",
              "           0.0855,  0.0307,  0.0189,  0.1379,  0.1158,  0.1468, -0.0483,\n",
              "          -0.1450,  0.0827, -0.0896,  0.0512],\n",
              "         [-0.2387,  0.1965,  0.0220, -0.0229,  0.0112,  0.1458,  0.0652,\n",
              "           0.1262, -0.2429, -0.2160,  0.1576, -0.1032, -0.0449, -0.1938,\n",
              "          -0.1887, -0.3613,  0.0441, -0.2639,  0.0496, -0.1305,  0.3170,\n",
              "           0.0427,  0.0478, -0.0279,  0.2045,  0.0940,  0.1653, -0.0401,\n",
              "          -0.0726,  0.0558, -0.1385,  0.0216],\n",
              "         [-0.1604,  0.2600,  0.0662,  0.0191, -0.0571,  0.0352,  0.0712,\n",
              "           0.0562, -0.1422, -0.2034,  0.1284, -0.1550, -0.0348, -0.1355,\n",
              "          -0.2781, -0.2689,  0.0417, -0.3377,  0.1139, -0.0518,  0.2045,\n",
              "           0.0747,  0.0600, -0.0621,  0.1358,  0.0932,  0.1610, -0.0561,\n",
              "          -0.1160,  0.0904, -0.0431,  0.0118]]], grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_output_tp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SFJUEqKWIaS",
        "outputId": "79d9128b-20d9-4165-ca36-b0c67c82adae"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.0735,  0.1547, -0.0292,  0.0470, -0.0282,  0.0597,  0.1018,\n",
              "           0.1307, -0.1112, -0.1699,  0.2083, -0.1083,  0.0874, -0.0449,\n",
              "          -0.1183, -0.2595, -0.0655, -0.2183,  0.1065, -0.1037,  0.2390,\n",
              "           0.0212,  0.0796, -0.0258,  0.1713,  0.0254,  0.1817, -0.1307,\n",
              "          -0.1476, -0.0997, -0.0394, -0.0292],\n",
              "         [-0.1030,  0.1681, -0.0376,  0.0200, -0.1233,  0.0678,  0.1338,\n",
              "          -0.0023, -0.1671, -0.1551,  0.1568, -0.0524, -0.0250, -0.0711,\n",
              "          -0.2904, -0.2652, -0.0289, -0.2211,  0.0744, -0.0724,  0.2301,\n",
              "           0.0566,  0.0180, -0.0022,  0.0772,  0.1041,  0.1453, -0.1092,\n",
              "          -0.1428,  0.0365,  0.0212, -0.0211],\n",
              "         [-0.1740,  0.0882, -0.0542, -0.0338, -0.0964,  0.1371,  0.0987,\n",
              "           0.0847, -0.2318, -0.1612,  0.1891, -0.0056, -0.0497, -0.1431,\n",
              "          -0.2468, -0.3641,  0.0449, -0.1987,  0.0127, -0.1358,  0.3252,\n",
              "           0.0855,  0.0307,  0.0189,  0.1379,  0.1158,  0.1468, -0.0483,\n",
              "          -0.1450,  0.0827, -0.0896,  0.0512],\n",
              "         [-0.2387,  0.1965,  0.0220, -0.0229,  0.0112,  0.1458,  0.0652,\n",
              "           0.1262, -0.2429, -0.2160,  0.1576, -0.1032, -0.0449, -0.1938,\n",
              "          -0.1887, -0.3613,  0.0441, -0.2639,  0.0496, -0.1305,  0.3170,\n",
              "           0.0427,  0.0478, -0.0279,  0.2045,  0.0940,  0.1653, -0.0401,\n",
              "          -0.0726,  0.0558, -0.1385,  0.0216],\n",
              "         [-0.1604,  0.2600,  0.0662,  0.0191, -0.0571,  0.0352,  0.0712,\n",
              "           0.0562, -0.1422, -0.2034,  0.1284, -0.1550, -0.0348, -0.1355,\n",
              "          -0.2781, -0.2689,  0.0417, -0.3377,  0.1139, -0.0518,  0.2045,\n",
              "           0.0747,  0.0600, -0.0621,  0.1358,  0.0932,  0.1610, -0.0561,\n",
              "          -0.1160,  0.0904, -0.0431,  0.0118]]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}